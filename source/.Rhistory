model {
// priors
y0 ~ normal(0, 10);
r ~ normal(0, 10);
K ~ normal(0, 10);
sigma ~ normal(0,10);
// likelihood
for (i in 1:n){
y[i] ~ normal( K/( 1+(K-y0)/y0 * exp(-r*t[i]) ), sigma );
}
}
'
stan_model = stan_model(model_code=stan_code)
fit  = sampling(stan_model,
data=data,
chains=3,
iter=2000,
warmup=1000
)
print(fit, digits=3, probs=c(0.025, 0.975))
# plot(fit)
plot(As.mcmc.list(fit)) # from coda package
posterior=as.matrix(fit)
correlationPlot(posterior[,1:4], thin=1)
t.pred = data$t
y.cred = matrix(0, nrow=nrow(posterior), ncol=length(t.pred))
for(i in 1:nrow(posterior)){
y.cred[i, ] = posterior[i,"K"]/( 1+(posterior[i,"K"]-posterior[i,"y0"])/posterior[i,"y0"] * exp(-posterior[i,"r"]*t.pred) )
}
plot(t,y)
y.cred.mean = apply(y.cred, 2, function(x) mean(x))
lines(t.pred, y.cred.mean, col="red", lwd=2)
y.cred.q05 = apply(y.cred, 2, function(x) quantile(x, probs=0.05))
lines(t.pred, y.cred.q05, col="red", lwd=2, lty=2)
y.cred.q95 = apply(y.cred, 2, function(x) quantile(x, probs=0.95))
lines(t.pred, y.cred.q95, col="red", lwd=2, lty=2)
y.pred = matrix(0, nrow=nrow(posterior), ncol=length(t.pred))
for(i in 1:nrow(posterior)){
y.pred[i, ] = rnorm(n=length(t.pred),
mean = posterior[i,"K"]/( 1+(posterior[i,"K"]-posterior[i,"y0"])/posterior[i,"y0"] * exp(-posterior[i,"r"]*t.pred) ),
sd = posterior[i,"sigma"])
}
y.pred.mean = apply(y.pred, 2, function(x) mean(x))
lines(t.pred, y.pred.mean, col="blue", lwd=2)
y.pred.q05 = apply(y.pred, 2, function(x) quantile(x, probs=0.05))
lines(t.pred, y.pred.q05, col="blue", lwd=2, lty=2)
y.pred.q95 = apply(y.pred, 2, function(x) quantile(x, probs=0.95))
lines(t.pred, y.pred.q95, col="blue", lwd=2, lty=2)
plot(data$y, y.pred.mean, ylim=c(min(data$y), max(data$y)))
abline(0,1)
for (i in 1:n){
lines(c(data$y[i], data$y[i]), c(y.pred.q05[i], y.pred.q95[i]))
}
#------------------------------------------------------------------------------
# lognormal residual model
#------------------------------------------------------------------------------
stan_code_lognormal = '
data {
int n;
real y[n];
real t[n];
}
parameters {
real<lower=0> y0;
real<lower=0> r;
real<lower=0> K;
real<lower=0> sigma;
}
model {
// priors
y0 ~ normal(0, 10);
r ~ normal(0, 10);
K ~ normal(0, 10);
sigma ~ normal(0,10);
// likelihood
for (i in 1:n){
y[i] ~ lognormal( log( K/( 1+(K-y0)/y0 * exp(-r*t[i]) ) ), sigma );
}
}
'
stan_model_lognormal = stan_model(model_code=stan_code_lognormal)
fit_lognormal  = sampling(stan_model_lognormal,
data=data,
chains=3,
iter=2000,
warmup=1000
)
print(fit_lognormal, digits=3, probs=c(0.025, 0.975))
# plot(fit_lognormal)
plot(As.mcmc.list(fit_lognormal)) # from coda package
posterior=as.matrix(fit_lognormal)
t.pred = data$t
y.cred = matrix(0, nrow=nrow(posterior), ncol=length(t.pred))
for(i in 1:nrow(posterior)){
y.cred[i, ] = posterior[i,"K"]/( 1+(posterior[i,"K"]-posterior[i,"y0"])/posterior[i,"y0"] * exp(-posterior[i,"r"]*t.pred) )
}
plot(t,y)
y.cred.mean = apply(y.cred, 2, function(x) mean(x))
lines(t.pred, y.cred.mean, col="red", lwd=2)
y.cred.q05 = apply(y.cred, 2, function(x) quantile(x, probs=0.05))
lines(t.pred, y.cred.q05, col="red", lwd=2, lty=2)
y.cred.q95 = apply(y.cred, 2, function(x) quantile(x, probs=0.95))
lines(t.pred, y.cred.q95, col="red", lwd=2, lty=2)
y.pred = matrix(0, nrow=nrow(posterior), ncol=length(t.pred))
for(i in 1:nrow(posterior)){
y.pred[i, ] = rlnorm(n=length(t.pred),
meanlog = log( posterior[i,"K"]/( 1+(posterior[i,"K"]-posterior[i,"y0"])/posterior[i,"y0"] * exp(-posterior[i,"r"]*t.pred) ) ),
sdlog = posterior[i,"sigma"])
}
y.pred.mean = apply(y.pred, 2, function(x) mean(x))
lines(t.pred, y.pred.mean, col="blue", lwd=2)
y.pred.q05 = apply(y.pred, 2, function(x) quantile(x, probs=0.05))
lines(t.pred, y.pred.q05, col="blue", lwd=2, lty=2)
y.pred.q95 = apply(y.pred, 2, function(x) quantile(x, probs=0.95))
lines(t.pred, y.pred.q95, col="blue", lwd=2, lty=2)
plot(data$y, y.pred.mean, ylim=c(min(data$y), max(data$y)))
abline(0,1)
for (i in 1:n){
lines(c(data$y[i], data$y[i]), c(y.pred.q05[i], y.pred.q95[i]))
}
plot(t,y, log="y")
lines(t.pred, y.cred.mean, col="red", lwd=2)
lines(t.pred, y.cred.q05, col="red", lwd=2, lty=2)
lines(t.pred, y.cred.q95, col="red", lwd=2, lty=2)
lines(t.pred, y.pred.mean, col="blue", lwd=2)
lines(t.pred, y.pred.q05, col="blue", lwd=2, lty=2)
lines(t.pred, y.pred.q95, col="blue", lwd=2, lty=2)
plot(data$y, y.pred.mean, ylim=c(min(data$y), max(data$y)), log="xy")
abline(0,1)
for (i in 1:n){
lines(c(data$y[i], data$y[i]), c(y.pred.q05[i], y.pred.q95[i]))
}
rm(list=ls())
library(rstan)
library(coda)
library(BayesianTools)
setwd("~/Desktop/teaching Bayes")
# Aim: learn what a random effect is.
# Learn about hierarchical levels of parameters. Bayesian statistics is great for hierearchical models!
# Think in terms of "no pooling", "partial pooling" and "complete pooling" of parameters
# rather than "random effects" and "fixed effects".
rstan_options(auto_write = TRUE)
options(mc.cores = 3)
df = read.csv("data/FruitflyDataReduced.csv")
setwd("~/git/bayesian-intro/source")
rm(list=ls())
library(rstan)
library(coda)
library(BayesianTools)
rstan_options(auto_write = TRUE)
options(mc.cores = 3)
df = read.csv("~/git/bayesian-intro/data/FruitflyDataReduced.csv")
head(df)
str(df)
boxplot(Longevity ~ CompanionNumber, data=df,
ylab="Longevity",
xlab="Companions ID",
col="grey")
plot(0, 0, xlim =c(1,5), ylim = range(df$Longevity), type = "n",
ylab="Longevity",
xlab="Companions ID")
points(Longevity ~ jitter(as.numeric(CompanionNumber), factor=0.2), data=df)
data = list(y = df$Longevity,
group = as.integer(df$CompanionNumber),
n = nrow(df),
n_group = 5)
data
stan_code_nopool = '
data {
int n;
int n_group;
real y[n];
int group[n];
}
parameters {
real<lower=0> mu[n_group];
real<lower=0> sigma;
}
model {
// priors
for (j in 1:n_group){
mu[j] ~ normal(0,100);
}
sigma ~ normal(0,100);
// likelihood
for(i in 1:n){
y[i] ~ normal(mu[group[i]], sigma);
}
}
'
stan_model_nopool = stan_model(model_code=stan_code_nopool)
stan_code_nopool = '
data {
int n;
int n_group;
real y[n];
int group[n];
}
parameters {
real<lower=0> mu[n_group];
real<lower=0> sigma;
}
model {
// priors
for (j in 1:n_group){
mu[j] ~ normal(0,100);
}
sigma ~ normal(0,100);
// likelihood
for(i in 1:n){
y[i] ~ normal(mu[group[i]], sigma);
}
}
'
fit_nopool  = sampling(stan_model_nopool,
data=data,
chains=3,
iter=2000,
warmup=1000
)
print(fit_nopool, digits=3, probs=c(0.025, 0.975))
plot(fit_nopool)
posterior_nopool = as.matrix(fit_nopool)
contrast = posterior_nopool[,"mu[4]"]-posterior_nopool[,"mu[5]"]
plot(density(contrast))
abline(v=0, col="red", lwd=2)
stan_code_partpool = '
data {
int n;
int n_group;
real y[n];
int group[n];
}
parameters {
real<lower=0> mu[n_group];
real<lower=0> sigma;
real<lower=0> mu_total;
real<lower=0> sigma_total;
}
model {
// priors
mu_total ~ normal(0, 100);
sigma_total ~ cauchy(0, 10);
for (j in 1:n_group){
mu[j] ~ normal(mu_total, sigma_total);
}
sigma ~ normal(0,100);
// likelihood
for(i in 1:n){
y[i] ~ normal(mu[group[i]], sigma);
}
}
'
stan_model_partpool = stan_model(model_code=stan_code_partpool)
fit_partpool  = sampling(stan_model_partpool,
data=data,
chains=3,
iter=2000,
warmup=1000
)
print(fit_partpool, digits=3, probs=c(0.025, 0.975))
plot(fit_partpool)
summary_nopool = summary(fit_nopool)$summary
summary_nopool
summary_partpool = summary(fit_partpool)$summary
summary_partpool
plot(0, 0, xlim =c(1,5), ylim = range(df$Longevity), type = "n",
ylab="Longevity",
xlab="Companions ID")
points(Longevity ~ jitter(as.numeric(CompanionNumber), factor=0.2), data=df,
col="grey")
points(summary_nopool[1:5, "mean"], pch="+", col="blue", cex=2)
points(summary_nopool[1:5, "2.5%"], pch="-", col="blue", cex=2)
points(summary_nopool[1:5, "97.5%"], pch="-", col="blue", cex=2)
points(summary_partpool[1:5, "mean"], pch="+", col="red", cex=2)
points(summary_partpool[1:5, "2.5%"], pch="-", col="red", cex=2)
points(summary_partpool[1:5, "97.5%"], pch="-", col="red", cex=2)
abline(h=mean(df$Longevity), col="grey")
legend("topright", legend=c("no pooling","partial pooling"), pch=c("+","+"), col=c("red","blue"), bty="n")
# The difference between "no pooling" and "partial pooling" estimates is that
# The difference between "no pooling" and "partial pooling" estimates is that
# extreme values tend to be pulled towards the joint mean by partial pooling (shrinkage).
# The difference between "no pooling" and "partial pooling" estimates is that
# extreme values tend to be pulled towards the joint mean by partial pooling (shrinkage).
# Statistical power is borrowed across groups.
# In addition to both models, there is also "complete pooling". Here, all information across groups would be pooled:
# In addition to both models, there is also "complete pooling". Here, all information across groups would be pooled:
# y_i ~ normal(mu, sigma)
# In addition to both models, there is also "complete pooling". Here, all information across groups would be pooled:
# y_i ~ normal(mu, sigma)
# There is no effect of group included, i.e. all groups share the same mean.
rm(list=ls())
library(rstan)
library(coda)
library(BayesianTools)
set.seed(123) # initiate random number generator for reproducability
rstan_options(auto_write = TRUE)
options(mc.cores = 3)
df = read.csv("~/git/bayesian-intro/data/FruitflyDataReduced.csv")
head(df)
str(df)
plot(df$Thorax, df$Longevity, col=as.factor(df$CompanionNumber))
df$Thorax.norm = as.numeric(scale(df$Thorax))
df$group = as.integer(df$CompanionNumber)
plot(df$Thorax.norm, df$Longevity, col=as.factor(df$CompanionNumber))
# as.integer(<factor>) codes groups in alphabetical order
table(df$group, df$CompanionNumber)
par(mfrow=c(2,3))
for (i in 1:5){
df.sub=subset(df, df$group==i)
plot(df.sub$Thorax.norm,  df.sub$Longevity,
xlim=range(df$Thorax.norm),
ylim=range(df$Longevity),
main=levels(df$CompanionNumber)[i]
)
}
data = list(y = df$Longevity,
x = df$Thorax.norm,
group = df$group,
n = nrow(df),
n_group = 5)
stan_code_partial = '
data {
int n;
int n_group;
real y[n];
real x[n];
int group[n];
}
parameters {
real a[n_group];
real b;
real<lower=0> sigma;
real mu_a;
real<lower=0> sigma_a;
}
model {
// priors
mu_a ~ normal(0,100);
sigma_a ~ cauchy(0,10);
for (j in 1:n_group){
a[j] ~ normal(mu_a,sigma_a);
}
b ~ normal(0,100);
sigma ~ normal(0,100);
// likelihood
for(i in 1:n){
y[i] ~ normal(a[group[i]]+b*x[i], sigma);
}
}
'
stan_model_partial = stan_model(model_code=stan_code_partial)
fit_partial  = sampling(stan_model_partial,
data=data,
chains=3,
iter=2000,
warmup=1000
)
print(fit_partial, digits=3, probs=c(0.025, 0.975))
plot(fit_partial)
posterior=as.matrix(fit_partial)
contrast = posterior[,"a[4]"]-posterior[,"a[5]"]
plot(density(contrast))
abline(v=0, col="red", lwd=2)
print(fit_partial, digits=3, probs=c(0.025, 0.975))
plot(fit_partial)
posterior=as.matrix(fit_partial)
contrast = posterior[,"a[4]"]-posterior[,"a[5]"]
plot(density(contrast))
par(mfrow=c(1,1))
plot(density(contrast))
abline(v=0, col="red", lwd=2)
x.pred = seq(from=min(df$Thorax.norm), to=max(df$Thorax.norm), by=0.1)
par(mfrow=c(2,3))
for (i in 1:5){
df.sub=subset(df, df$group==i)
plot(df.sub$Thorax.norm,  df.sub$Longevity,
xlim=range(df$Thorax.norm),
ylim=range(df$Longevity),
main=levels(df$CompanionNumber)[i]
)
y.cred = matrix(0, nrow=nrow(posterior), ncol=length(x.pred))
for(j in 1:nrow(posterior)){
# column i in posterior corresponds to a_i, alternatively reference by name: posterior[j,paste0("a[",i,"]")]
y.cred[j, ] = posterior[j,i] + posterior[j,"b"]*x.pred
}
y.cred.mean = apply(y.cred, 2, function(x) mean(x))
lines(x.pred, y.cred.mean, col="red", lwd=2)
y.cred.q05 = apply(y.cred, 2, function(x) quantile(x, probs=0.05))
lines(x.pred, y.cred.q05, col="red", lwd=2, lty=2)
y.cred.q95 = apply(y.cred, 2, function(x) quantile(x, probs=0.95))
lines(x.pred, y.cred.q95, col="red", lwd=2, lty=2)
}
rm(list=ls())
library(rstan)
library(coda)
library(BayesianTools)
# setwd("~/Desktop/teaching Bayes")
set.seed(123) # initiate random number generator for reproducability
rstan_options(auto_write = TRUE)
options(mc.cores = 3)
# In the previous model, we just fitted mean values to groups and there was still some unexplained variation.
# Here, we will add a continuous predictor (covariate).
# Specifically, for the same dataset of fruitfly longevity and sexual activity, we add the covariate individual body size.
# Lifespan is assumed to be positively correlated with body size.
#------------------------------------------------------------------------------
# load data
#------------------------------------------------------------------------------
df = read.csv("~/git/bayesian-intro/data/FruitflyDataReduced.csv")
df$Thorax.norm = as.numeric(scale(df$Thorax))
df$group = as.integer(df$CompanionNumber)
data = list(y = df$Longevity,
x = df$Thorax.norm,
group = df$group,
n = nrow(df),
n_group = 5)
par(mfrow=c(2,3))
for (i in 1:5){
df.sub=subset(df, df$group==i)
plot(df.sub$Thorax.norm,  df.sub$Longevity,
xlim=range(df$Thorax.norm),
ylim=range(df$Longevity),
main=levels(df$CompanionNumber)[i]
)
}
#------------------------------------------------------------------------------
# partial pooling model
#------------------------------------------------------------------------------
# We will fit linear regression lines to each group ("CompanionNumber) as follows:
# y_i ~ normal(a[group_i]+b[group_i]*x_i, sigma) i=1,...,n (n observations)
# a_j ~ normal(mu_a, sigma_a) j=1,...,m (m groups)
# b_j ~ normal(mu_b, sigma_b) j=1,...,m (m groups)
# Here, a_j and b_j are group-level intercepts and slopes,
# which are allowed to vary (partial pooling).
# Both have their own means and standard deviations, which are also free parameters to be estimated.
# So this is a random intercepts and slopes linear regression, lm-formulation would be `y ~ x + (x|group)`,
# which is short for `y ~ 1+x + (1+x|group)`.
stan_code_partial = '
data {
int n;
int n_group;
real y[n];
real x[n];
int group[n];
}
parameters {
real a[n_group];
real b[n_group];
real<lower=0> sigma;
real mu_a;
real<lower=0> sigma_a;
real mu_b;
real<lower=0> sigma_b;
}
model {
// priors
mu_a ~ normal(0,100);
mu_b ~ normal(0,10);
sigma_a ~ cauchy(0,10);
sigma_b ~ cauchy(0,10);
for (j in 1:n_group){
a[j] ~ normal(mu_a,sigma_a);
b[j] ~ normal(mu_b,sigma_b);
}
sigma ~ normal(0,100);
// likelihood
for(i in 1:n){
y[i] ~ normal(a[group[i]]+b[group[i]]*x[i], sigma);
}
}
'
indicates
stan_model_partial = stan_model(model_code=stan_code_partial)
fit_partial  = sampling(stan_model_partial,
data=data,
chains=3,
iter=2000,
warmup=1000
)
fit_partial  = sampling(stan_model_partial,
data=data,
chains=3,
iter=2000,
warmup=1000,
control=list(adapt_delta=0.999, max_treedepth=12)
)
print(fit_partial, digits=3, probs=c(0.025, 0.975))
plot(fit_partial)
# plot(fit_partial, pars="a")
plot(As.mcmc.list(fit_partial)) # from coda package
pairs(fit_partial, pars=c("b","mu_b","sigma_b"))
posterior=as.matrix(fit_partial)
x.pred = seq(from=min(df$Thorax.norm), to=max(df$Thorax.norm), by=0.1)
par(mfrow=c(2,3))
for (i in 1:5){
df.sub=subset(df, df$group==i)
plot(df.sub$Thorax.norm,  df.sub$Longevity,
xlim=range(df$Thorax.norm),
ylim=range(df$Longevity),
main=levels(df$CompanionNumber)[i]
)
y.cred = matrix(0, nrow=nrow(posterior), ncol=length(x.pred))
for(j in 1:nrow(posterior)){
y.cred[j, ] = posterior[j,paste0("a[",i,"]")] + posterior[j,paste0("b[",i,"]")]*x.pred
}
y.cred.mean = apply(y.cred, 2, function(x) mean(x))
lines(x.pred, y.cred.mean, col="red", lwd=2)
y.cred.q05 = apply(y.cred, 2, function(x) quantile(x, probs=0.05))
lines(x.pred, y.cred.q05, col="red", lwd=2, lty=2)
y.cred.q95 = apply(y.cred, 2, function(x) quantile(x, probs=0.95))
lines(x.pred, y.cred.q95, col="red", lwd=2, lty=2)
}
